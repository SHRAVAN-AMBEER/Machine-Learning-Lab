{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHRAVAN-AMBEER/Machine-Learning-Lab/blob/main/week9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.)Simple Perceptron"
      ],
      "metadata": {
        "id": "wEMr3xn6gJFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHrYcKrgEys",
        "outputId": "bf4adaa0-6395-4c7e-d49b-afa531398d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron output: 0.9999938558253978\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "# Example\n",
        "weights = np.array([0, 3])\n",
        "bias = 3\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])  # input\n",
        "print(\"Neuron output:\", n.feedforward(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)Perceptron with activation Function with AND, OR, XOR"
      ],
      "metadata": {
        "id": "QxF-n88ggVo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=10):\n",
        "        #learning_rate: how much to adjust weights when an error occurs\n",
        "        self.learning_rate = learning_rate\n",
        "        #epochs: how many times to iterate over the entire dataset.\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        #If the weighted sum ≥ 0 ⇒ output = 1 else 0\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n_samples):\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "                y_pred = self.step_function(linear_output)\n",
        "                update = self.learning_rate * (y[i] - y_pred)\n",
        "                self.weights += update * X[i]\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.step_function(linear_output)\n",
        "\n",
        "\n",
        "#AND Gate\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_and = np.array([0,0,1,1])\n",
        "p_and = Perceptron()\n",
        "p_and.fit(X, y_and)\n",
        "print(\"AND Predictions:\", p_and.predict(X))\n",
        "\n",
        "#OR Gate\n",
        "y_or = np.array([0,1,0,1])\n",
        "p_or = Perceptron()\n",
        "p_or.fit(X, y_or)\n",
        "print(\"OR Predictions:\", p_or.predict(X))\n",
        "\n",
        "#XOR Gate (not linearly separable, will fail intentionally)no single straight line can divide 1s and 0s we need at least two layers (an MLP)\n",
        "y_xor = np.array([0,1,1,0])\n",
        "p_xor = Perceptron()\n",
        "p_xor.fit(X, y_xor)\n",
        "print(\"XOR Predictions:\", p_xor.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6UFIOlogVJ9",
        "outputId": "62d84510-5c23-489b-e68b-2fe3adf4ad8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Predictions: [0 0 1 1]\n",
            "OR Predictions: [0 1 0 1]\n",
            "XOR Predictions: [1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.)MLP (Multi-Layer Perceptron) with Single Hidden Layer Given: weights = [0, 1] bias = 0 x = [2, 3] sigmoid(z) = 1 / (1 + exp(-z))\n",
        "\n",
        "Step 1: hidden neuron h1 total_h1 = 02 + 13 + 0 = 3 out_h1 = sigmoid(3) = 1 / (1 + e^(-3)) = 0.9525741268224334\n",
        "\n",
        "Step 2: hidden neuron h2 (same weights) total_h2 = 3 out_h2 = sigmoid(3) = 0.9525741268224334\n",
        "\n",
        "Step 3: output neuron o1 (inputs = [out_h1, out_h2]) total_o1 = 0out_h1 + 1out_h2 + 0 = 0.9525741268224334 out_o1 = sigmoid(total_o1) = sigmoid(0.9525741268224334) = 1 / (1 + e^(-0.9525741268224334)) = 0.7216325609518421\n",
        "\n",
        "Final network output: 0.7216325609518421\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UuHjNSZJgat9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "    \"\"\"\n",
        "    Neural Network with:\n",
        "    - 2 input neurons\n",
        "    - 2 hidden neurons (h1, h2)\n",
        "    - 1 output neuron (o1)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        weights = np.array([0, 1])\n",
        "        bias = 0\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "        return out_o1\n",
        "\n",
        "# Example\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(\"Network output:\", network.feedforward(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD7HSp1dgaVO",
        "outputId": "634b709f-3711-4ceb-d11f-cbfce0fa736e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output: 0.7216325609518421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST MLP(Multi-Layer perceptron) Classifier"
      ],
      "metadata": {
        "id": "78u6ToZuggY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml                # To load the MNIST dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "# Each pixel value ranges from 0 to 255 — normalizing them to 0–1 for faster training\n",
        "data = data / 255.0\n",
        "# stratify=labels --> ensures equal class distribution in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Initializing the Multi-Layer Perceptron (MLP)\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(25,),   # One hidden layer with 25 neurons\n",
        "    max_iter=25,                # Maximum 25 training iterations (epochs)\n",
        "    verbose=True,               # Prints training progress and loss values\n",
        "    random_state=1              # For reproducibility of weight initialization\n",
        ")\n",
        "mlp.fit(X_train, y_train)\n",
        "# .predict() --> predicts labels for test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvTbXIh0gg-Q",
        "outputId": "3f13f065-cc9c-4c59-e2df-bef825c37f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61783287\n",
            "Iteration 2, loss = 0.29524015\n",
            "Iteration 3, loss = 0.25120147\n",
            "Iteration 4, loss = 0.22306254\n",
            "Iteration 5, loss = 0.20162152\n",
            "Iteration 6, loss = 0.18372515\n",
            "Iteration 7, loss = 0.17067816\n",
            "Iteration 8, loss = 0.15897896\n",
            "Iteration 9, loss = 0.14903345\n",
            "Iteration 10, loss = 0.14075923\n",
            "Iteration 11, loss = 0.13318629\n",
            "Iteration 12, loss = 0.12669473\n",
            "Iteration 13, loss = 0.12190370\n",
            "Iteration 14, loss = 0.11634222\n",
            "Iteration 15, loss = 0.11137318\n",
            "Iteration 16, loss = 0.10698084\n",
            "Iteration 17, loss = 0.10291621\n",
            "Iteration 18, loss = 0.09961420\n",
            "Iteration 19, loss = 0.09588943\n",
            "Iteration 20, loss = 0.09264285\n",
            "Iteration 21, loss = 0.08990875\n",
            "Iteration 22, loss = 0.08734154\n",
            "Iteration 23, loss = 0.08420773\n",
            "Iteration 24, loss = 0.08217381\n",
            "Iteration 25, loss = 0.08031738\n",
            "Test Accuracy: 0.9615714285714285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}